batch_size: 512
val_split: 0.1
loss: l1
pretrained_id: y9qwghft
lr: 0.0001
lr_scheduler: False
dataset:
  _target_: data.diffusion.GRUDiffusionDatasetConfig
  context_length: [1, 50]
  pretrained_embedding: True
  new_encoder: True
diffusion:
  latent_shape: [1,512]
  sampling_schedule: null
  sampling_timesteps: 100
  seq_conditional: True
  seq_conditional_dim: 384
  seq_unconditional_prob: 0.1
  class_conditional: False
  num_classes: 0
  class_unconditional_prob: 0.1
  self_condition: False
  train_prob_self_cond: 0.5
  train_schedule: cosine
  objective: pred_v
  scale: 1.0
  sampler: ddpm
  normalize_latent: False
  # Transformer-specific config
  n_layers: 6
  n_heads: 8
  scale_shift: True
  num_dense_connections: 3
  dropout: 0.0
  cond_encoder_kwargs:
    n_embd: 384
    n_layers: 3
    n_heads: 6
    vocab_size: 50