batch_size: 512
val_split: 0.1
loss: l1
pretrained_id: y9qwghft
lr: 0.0001
lr_scheduler: True
dataset:
  context_length: [200, 200]
  cond_tokens_type: pretrained
  latent_type: known_encoder_new
diffusion:
  latent_shape: [1,256]
  sampling_schedule: null
  sampling_timesteps: 250
  seq_conditional: True
  seq_conditional_dim: 384
  seq_unconditional_prob: 0.1
  class_conditional: False
  num_classes: 0
  class_unconditional_prob: 0.1
  self_condition: True
  train_prob_self_cond: 0.5
  train_schedule: cosine
  objective: pred_v
  scale: 1.0
  sampler: ddpm
  normalize_latent: True
  # Transformer-specific config
  n_embd: 256
  n_layers: 6
  n_heads: 8
  scale_shift: True
  num_dense_connections: 3
  dropout: 0.0
  cond_encoder_kwargs: null