data : 
  n_latents: 22
  n_states: 66
  n_overlap: 1
  min_active_latents: 6
  max_active_latents: 16
  context_length: 50

model:
  block_size: 200
  vocab_size: 67 # data.n_states + 1, FIX that at some point
  n_layer: 4
  n_head: 4
  n_embd: 64
  dropout: 0.1
  bias: False

val_ratio: 0.2
batch_size: 256
lr: 1e-3