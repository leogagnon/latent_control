data : 
  n_latents: 22
  n_states: 66
  n_overlap: 1
  min_active_latents: 6
  max_active_latents: 16
  context_length: 150
  seed: 42

model:
  block_size: 200
  vocab_size: 67 # data.n_states + 1, FIX that at some point
  n_layer: 6
  n_head: 6
  n_embd: 216
  dropout: 0.1
  bias: False

val_size: 1000
batch_size: 1024
lr: 1e-3