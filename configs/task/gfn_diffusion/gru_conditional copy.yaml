batch_size: 512
val_split: 0.1
loss: l1
pretrained_id: 3gx9owf7
lr: 0.0001
lr_scheduler: False
log_var_range: 4
t_scale: 1
trajectory_length: 
model:
  latent_shape: (1, 128)
  n_layers: 4
  n_heads: 6
  dropout: 0.0
  scale_shift: True
  num_dense_connections: 3
  cond_encoder_kwargs: null
  seq_conditional: True
  seq_conditional_dim: 256
  langevin: True
  lgv_clip: 1e2
  gfn_clip: 1e4
  learned_variance: True
dataset:
  _target_: data.diffusion.GRUDiffusionDatasetConfig
  context_length: [200, 200]
  pretrained_embedding: False
  pretrained_embedding_id: null #y9qwghft
  suffix_size: [1,30]
diffusion:
  latent_shape: [6,384]
  sampling_schedule: null
  sampling_timesteps: 100
  seq_conditional: True
  seq_conditional_dim: 384
  seq_unconditional_prob: 0.1
  class_conditional: False
  num_classes: 0
  class_unconditional_prob: 0.1
  self_condition: True
  train_prob_self_cond: 0.5
  train_schedule: cosine
  objective: pred_v
  scale: 1.0
  sampler: ddpm
  normalize_latent: True
  # Transformer-specific config
  n_layers: 5
  n_heads: 8
  scale_shift: True
  num_dense_connections: 3
  dropout: 0.0
  cond_encoder_kwargs:
    n_embd: 384
    n_layers: 3
    n_heads: 6
    vocab_size: 50