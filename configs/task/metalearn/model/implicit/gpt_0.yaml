tag: gpt_0
encoder:
  _target_: models.encoder.TransformerEncoder
  max_seq_len: 300
  num_tokens : ${task.metalearn.data.n_obs}
  n_layer: 2
  n_head: 6
  n_embd: 32
  causal_mask: True
  sin_posemb: True
  bottleneck: False