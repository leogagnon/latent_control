tag: explicit_gpt10M
encoder:
  _target_: models.encoder.TransformerEncoder
  max_seq_len: 300
  num_tokens: ${eval:'${task.metalearn.data.n_obs} + 1'}
  n_layer: 4
  n_head: 8
  n_embd: 256
  causal_mask: True
  positional_encodings: True
  
decoder:
  _target_: models.decoder.TransformerDecoder
  max_seq_len: 300
  num_tokens : ${eval:'${task.metalearn.data.n_obs} + 1'}
  n_layer: 4
  n_head: 8
  n_embd: 256
  causal_mask: False
  full_context: False
  positional_encodings: True