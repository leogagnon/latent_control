tag: gpt_bn
encoder:
  _target_: models.encoder.ContextEncoder
  trainable: True
  context_is_prefix: False
  context_length: 100
  normalize: False
  layernorm: True
  pool_last_n: null
  backbone:
    _target_: models.encoder.TransformerEncoder
    max_seq_len: 300
    num_tokens : ${task.metalearn.data.n_obs}
    n_layer: 6
    n_head: 8
    n_embd: 384
    causal_mask: False
    sin_posemb: True
    bottleneck: True
    bottleneck_size: 6
    out_dim: 128

decoder:
  _target_: models.decoder.TransformerDecoder
  max_seq_len: 300
  num_tokens: ${task.metalearn.data.n_obs}
  n_layer: 4
  n_head: 6
  n_embd: 128
  soft_token_enc: True 
  causal_mask: True 
  sin_posemb: True 