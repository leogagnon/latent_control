batch_size: 512
val_split: 0.1
pretrained_id: 1agkm0yi # explicit_gpt10M
lr: 0.0001
loss: l2
train_schedule: cosine
diffusion_objective: pred_v
tag: explicit_large

model:
  n_embd: 512
  n_layers: 8
  n_heads: 8
  dropout: 0.0
  scale_shift: True
  seq_conditional: True
  seq_unconditional_prob: 0.0
  cond_modulation: True
  self_condition: False
  cond_encoder_kwargs:
    n_layers: 3
    n_heads: 8
    vocab_size: null
  
dataset:
  _target_: data.diffusion.ExplicitDiffusionDatasetConfig
  context_length: [190, 190]
  suffix_size: [1,30]
  pretrained_embedding: True
  pretrained_embedding_id: q4bv9oa7 #gpt5M
  cond_hidden: False