batch_size: 512
val_split: 0.1
pretrained_id: pe5m8x3i # gru5M
lr: 0.0001
loss: l2
train_schedule: cosine
diffusion_objective: pred_v
tag: gru_large

model:
  n_embd: 512
  n_layers: 8
  n_heads: 8
  dropout: 0.0
  scale_shift: True
  seq_conditional: True
  seq_unconditional_prob: 0.0
  cond_modulation: True
  self_condition: False
  cond_encoder_kwargs:
    n_layers: 3
    n_heads: 8
    vocab_size: null
  
dataset:
  _target_: data.diffusion.GRUDiffusionDatasetConfig
  context_length: [200, 200]
  suffix_size: [1,30]
  pretrained_embedding: True
  pretrained_embedding_id: q4bv9oa7 # gpt5M
  cond_hidden: False