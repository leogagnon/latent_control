{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(numpy, tp_name):\n",
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mila/l/leo.gagnon/latent_control')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from lightning_modules.diffusion_prior import DiffusionPriorTask\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from data.diffusion import KnownLatentDiffusionDataset, KnownLatentDiffusionDatasetConfig\n",
    "from models.encoder import DiffusionEncoder\n",
    "from torch2jax import j2t, t2j\n",
    "import lightning as L\n",
    "from models.x_transformer import Encoder, ScaledSinusoidalEmbedding\n",
    "from lightning_modules.metalearn import MetaLearningTask\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from einops import rearrange, repeat\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicEncoder(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_id: str,\n",
    "        n_embd: int,\n",
    "        batch_size: int,\n",
    "        val_split: float,\n",
    "        lr: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.val_split = val_split\n",
    "        self.lr = lr\n",
    "        self.n_embd = n_embd\n",
    "\n",
    "        self.base_task = MetaLearningTask(pretrained_id)\n",
    "        for param in self.base_task.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.seq_conditional_encoder = Encoder(\n",
    "            dim=n_embd,\n",
    "            depth=3,\n",
    "            heads=6,\n",
    "            attn_dropout=0.0,  # dropout post-attention\n",
    "            ff_dropout=0.0,  # feedforward dropout\n",
    "            rel_pos_bias=False,\n",
    "            ff_glu=True,\n",
    "        )\n",
    "\n",
    "        self.latent_model = Encoder(\n",
    "            dim=n_embd,\n",
    "            depth=3,\n",
    "            heads=6,\n",
    "            attn_dropout=0.0,  # dropout post-attention\n",
    "            ff_dropout=0.0,  # feedforward dropout\n",
    "            rel_pos_bias=False,\n",
    "            ff_glu=True,\n",
    "            cross_attend=True,\n",
    "        )\n",
    "        self.null_embedding = nn.Embedding(1, n_embd)\n",
    "        # self.out_proj = nn.Linear(n_embd, self.base_task.model.encoder.cfg.n_embd)\n",
    "        self.out_proj = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(n_embd, embedding.num_embeddings)\n",
    "                for embedding in self.base_task.model.encoder.latent_embedding\n",
    "            ]\n",
    "        )\n",
    "        self.seq_conditional_emb = nn.Embedding(\n",
    "            num_embeddings=50,\n",
    "            embedding_dim=n_embd,\n",
    "        )\n",
    "        self.seq_conditional_posemb = ScaledSinusoidalEmbedding(n_embd)\n",
    "        self.norm = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def setup(self, stage):\n",
    "        with torch.no_grad():\n",
    "            self.train_data = KnownLatentDiffusionDataset(\n",
    "                KnownLatentDiffusionDatasetConfig(context_length=[200, 200]),\n",
    "                self.base_task,\n",
    "                None,\n",
    "            )\n",
    "\n",
    "            # self.train_data, self.val_data = random_split(\n",
    "            #    dataset, [1 - self.val_split, self.val_split]\n",
    "            # )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        return opt\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=lambda x: x,\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch, batch_idx=None):\n",
    "        latent, raw_latent = batch[\"latent\"], batch[\"raw_latent\"]\n",
    "        cond_input_ids = batch[\"cond_input_ids\"]\n",
    "\n",
    "        cond = self.seq_conditional_emb(cond_input_ids)\n",
    "        cond = cond + self.seq_conditional_posemb(cond)\n",
    "        cond = self.seq_conditional_encoder(cond)\n",
    "\n",
    "        init_emb = repeat(self.null_embedding.weight, \"1 d -> b 1 d\", b=latent.shape[0])\n",
    "\n",
    "        pred = self.latent_model(\n",
    "            init_emb,\n",
    "            context=cond,\n",
    "        )\n",
    "        pred = self.norm(pred)\n",
    "        pred = [proj(pred) for proj in self.out_proj]\n",
    "\n",
    "        loss = sum(\n",
    "            [\n",
    "                nn.functional.cross_entropy(pred[i].squeeze(), raw_latent[:, i]).mean()\n",
    "                for i in range(len(pred))\n",
    "            ]\n",
    "        )\n",
    "        acc = sum(\n",
    "            [\n",
    "                (pred[i].squeeze().argmax(1) == raw_latent[:, i]).float().mean()\n",
    "                for i in range(len(pred))\n",
    "            ]\n",
    "        ) / len(pred)\n",
    "        # loss = torch.mean((pred - latent)**2)\n",
    "\n",
    "        self.log(\n",
    "            \"train/loss\",\n",
    "            loss.detach().cpu().numpy().item(),\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        self.log(\n",
    "            \"train/acc\",\n",
    "            acc.detach().cpu().numpy().item(),\n",
    "            prog_bar=True,\n",
    "        )\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/lightning/fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/mila/l/leo.gagnon/latent_control/venv/lib/pyth ...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "        max_steps=10000,\n",
    "        accelerator='gpu',\n",
    "        enable_checkpointing=False,\n",
    "        val_check_interval=100,\n",
    "        reload_dataloaders_every_n_epochs=1,\n",
    "        check_val_every_n_epoch=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset : (11288/1000)\n",
      "Loaded checkpoing : last.ckpt\n"
     ]
    }
   ],
   "source": [
    "deterministic_encoder = DeterministicEncoder(\"ekly943l\", 512, 512, 0.1, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                    | Type                      | Params | Mode \n",
      "------------------------------------------------------------------------------\n",
      "0 | base_task               | MetaLearningTask          | 6.3 M  | train\n",
      "1 | seq_conditional_encoder | Encoder                   | 11.8 M | train\n",
      "2 | latent_model            | Encoder                   | 14.2 M | train\n",
      "3 | null_embedding          | Embedding                 | 512    | train\n",
      "4 | out_proj                | ModuleList                | 13.9 K | train\n",
      "5 | seq_conditional_emb     | Embedding                 | 25.6 K | train\n",
      "6 | seq_conditional_posemb  | ScaledSinusoidalEmbedding | 1      | train\n",
      "7 | norm                    | LayerNorm                 | 1.0 K  | train\n",
      "------------------------------------------------------------------------------\n",
      "26.0 M    Trainable params\n",
      "6.3 M     Non-trainable params\n",
      "32.4 M    Total params\n",
      "129.500   Total estimated model params size (MB)\n",
      "345       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c259d5b4ae14a4e8a2d9308d32d9990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/l/leo.gagnon/latent_control/data/hmm.py:585: FutureWarning: None encountered in jnp.array(); this is currently treated as NaN. In the future this will result in an error.\n",
      "  intv_envs = jnp.array(intv_envs)\n",
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:4252: FutureWarning: None encountered in jnp.array(); this is currently treated as NaN. In the future this will result in an error.\n",
      "  return array(a, dtype=dtype, copy=bool(copy), order=order, device=device)\n",
      "/home/mila/l/leo.gagnon/latent_control/data/hmm.py:585: FutureWarning: None encountered in jnp.array(); this is currently treated as NaN. In the future this will result in an error.\n",
      "  intv_envs = jnp.array(intv_envs)\n",
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (24) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(deterministic_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic_encoder.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randperm(len(deterministic_encoder.train_data))[:512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/l/leo.gagnon/latent_control/data/hmm.py:585: FutureWarning: None encountered in jnp.array(); this is currently treated as NaN. In the future this will result in an error.\n",
      "  intv_envs = jnp.array(intv_envs)\n"
     ]
    }
   ],
   "source": [
    "raw_latent, latent, cond_input_ids, _ = deterministic_encoder.train_data.__getitems__(idx).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = deterministic_encoder.seq_conditional_emb(cond_input_ids)\n",
    "cond = cond + deterministic_encoder.seq_conditional_posemb(cond)\n",
    "\n",
    "init_emb = repeat(deterministic_encoder.null_embedding.weight, \"1 d -> b 1 d\", b=latent.shape[0])\n",
    "\n",
    "pred = deterministic_encoder.encoder(\n",
    "        init_emb,\n",
    "        context=cond,\n",
    "    )\n",
    "pred = deterministic_encoder.norm(pred)\n",
    "pred = [proj(pred) for proj in deterministic_encoder.out_proj]\n",
    "\n",
    "loss = sum([nn.functional.cross_entropy(pred[i].squeeze(), raw_latent[:,i]).mean() for i in range(len(pred))])\n",
    "acc = [(pred[i].squeeze().argmax(1) == raw_latent[:,i]).float().mean(0) for i in range(len(pred))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = deterministic_encoder.base_task.full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "oracle_out = dataset.bayesian_oracle(jnp.arange(len(dataset)), t2j(cond_input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array(9667, dtype=int32), tensor(9667))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.exp(oracle_out['log_alpha_post'][-1]).argmax(), idx[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
