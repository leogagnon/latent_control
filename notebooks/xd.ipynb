{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mila/l/leo.gagnon/latent_control')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tasks.metalearn import MetaLearningTask, MetaLearningConfig\n",
    "from models.decoder import TransformerDecoder, TransformerDecoderConfig\n",
    "from tasks.dsm_diffusion import DSMDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {1: 2, 3:4}\n",
    "a.update({'lol': 'xd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 5, 7])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10)[1:-2:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([[1,2,3], [4,5,6]])\n",
    "b = torch.Tensor([0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, d = a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [4.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.index_select(a, -1, torch.LongTensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLongTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as input tensor"
     ]
    }
   ],
   "source": [
    "torch.gather(a, 0, torch.LongTensor([0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xd.HI\n",
      "xd.BYE\n"
     ]
    }
   ],
   "source": [
    "for i in Enum('xd', {'HI': 0, 'BYE': 1}):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 2., 0., 3., 0.],\n",
       "        [4., 0., 5., 0., 6., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearrange([a, repeat(b, 'l -> b l', b=2)], 't b l -> b (l t)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 2., 2., 3., 3.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.repeat_interleave(torch.Tensor([1,2,3]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:191: Found keys that are in the model state dict but not in the checkpoint: ['seen_tokens', 'val_latents', 'train_latents']\n",
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:191: Found keys that are in the model state dict but not in the checkpoint: ['seen_tokens', 'val_latents', 'train_latents']\n",
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:191: Found keys that are in the model state dict but not in the checkpoint: ['seen_tokens', 'val_latents', 'train_latents']\n"
     ]
    }
   ],
   "source": [
    "task = DSMDiffusion.load_from_checkpoint(\n",
    "    \"/network/scratch/l/leo.gagnon/latent_control_log/checkpoints/f3oppg5h/last.ckpt\", strict=False\n",
    ")\n",
    "task.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = TransformerDecoder(TransformerDecoderConfig(300,50,15,8,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xd.attn_layers.layers[0][1].to_q.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157.497345"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(xd) / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_target_: models.decoder.TransformerDecoder\n",
    "  max_seq_len: 300\n",
    "  num_tokens : ${eval:'${task.metalearn.data.n_obs} + 1'}\n",
    "  n_layer: 10\n",
    "  n_head: 8\n",
    "  n_embd: 512\n",
    "  positional_encodings: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = MetaLearningTask.from_id('8jzqijc5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/'.join([task.wandb_dict['ckpts_dir'], task.wandb_dict['ckpts_names'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaLearningTask(\n",
       "  (model): MetaLearner(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (token_emb): TokenEmbedding(\n",
       "        (emb): Embedding(51, 384)\n",
       "      )\n",
       "      (pos_emb): ScaledSinusoidalEmbedding()\n",
       "      (post_emb_norm): Identity()\n",
       "      (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (project_emb): Identity()\n",
       "      (attn_layers): Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n",
       "              (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "              (attend): Attend(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (ff): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n",
       "              (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "              (attend): Attend(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (ff): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (4): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n",
       "              (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "              (attend): Attend(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (5): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (ff): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (6): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n",
       "              (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "              (attend): Attend(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (7): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (ff): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (8): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n",
       "              (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "              (attend): Attend(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (9): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (ff): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (10): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n",
       "              (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "              (attend): Attend(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (11): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (ff): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "        )\n",
       "        (adaptive_mlp): Identity()\n",
       "        (final_norm): LayerNorm(\n",
       "          (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "        )\n",
       "        (skip_combines): ModuleList(\n",
       "          (0-11): 12 x None\n",
       "        )\n",
       "      )\n",
       "      (to_logits): Linear(in_features=384, out_features=51, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MetaLearningTask.load_from_checkpoint(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    d = 10\n",
    "    def __init__(self, a):\n",
    "        self.a = a\n",
    "    \n",
    "    def xd(self):\n",
    "        return self.__class__\n",
    "\n",
    "class B(A):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.B at 0x7fdfeb59a4d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.xd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
