{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mila/l/leo.gagnon/latent_control')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from tasks.metalearn import MetaLearningTask, MetaLearningConfig\n",
    "from models.decoder import TransformerDecoder, TransformerDecoderConfig\n",
    "from tasks.dsm_diffusion import DSMDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:191: Found keys that are in the model state dict but not in the checkpoint: ['seen_tokens', 'val_latents', 'train_latents']\n",
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:191: Found keys that are in the model state dict but not in the checkpoint: ['seen_tokens', 'val_latents', 'train_latents']\n",
      "/home/mila/l/leo.gagnon/latent_control/venv/lib/python3.10/site-packages/lightning/pytorch/core/saving.py:191: Found keys that are in the model state dict but not in the checkpoint: ['seen_tokens', 'val_latents', 'train_latents']\n"
     ]
    }
   ],
   "source": [
    "task = DSMDiffusion.load_from_checkpoint(\n",
    "    \"/network/scratch/l/leo.gagnon/latent_control_log/checkpoints/f3oppg5h/last.ckpt\", strict=False\n",
    ")\n",
    "task.cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "del task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "xd = TransformerDecoder(TransformerDecoderConfig(300,50,15,8,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xd.attn_layers.layers[0][1].to_q.weight.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157.497345"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(xd) / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_target_: models.decoder.TransformerDecoder\n",
    "  max_seq_len: 300\n",
    "  num_tokens : ${eval:'${task.metalearn.data.n_obs} + 1'}\n",
    "  n_layer: 10\n",
    "  n_head: 8\n",
    "  n_embd: 512\n",
    "  positional_encodings: True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = MetaLearningTask.from_id('8jzqijc5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/'.join([task.wandb_dict['ckpts_dir'], task.wandb_dict['ckpts_names'][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetaLearningTask(\n",
       "  (model): MetaLearner(\n",
       "    (decoder): TransformerDecoder(\n",
       "      (token_emb): TokenEmbedding(\n",
       "        (emb): Embedding(51, 384)\n",
       "      )\n",
       "      (pos_emb): ScaledSinusoidalEmbedding()\n",
       "      (post_emb_norm): Identity()\n",
       "      (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (project_emb): Identity()\n",
       "      (attn_layers): Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n",
       "              (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "              (attend): Attend(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (1): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (ff): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (2): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n",
       "              (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "              (attend): Attend(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (3): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (ff): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (4): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n",
       "              (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "              (attend): Attend(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (5): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (ff): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (6): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n",
       "              (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "              (attend): Attend(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (7): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (ff): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (8): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n",
       "              (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "              (attend): Attend(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (9): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (ff): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (10): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): Attention(\n",
       "              (to_q): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_k): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (to_v): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (split_q_heads): Rearrange('b n (h d) -> b h n d', h=8)\n",
       "              (split_k_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (split_v_heads): Rearrange('b n (h d) -> b h n d', d=64)\n",
       "              (merge_heads): Rearrange('b h n d -> b n (h d)')\n",
       "              (attend): Attend(\n",
       "                (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (to_out): Linear(in_features=512, out_features=384, bias=False)\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "          (11): ModuleList(\n",
       "            (0): ModuleList(\n",
       "              (0): LayerNorm(\n",
       "                (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "              )\n",
       "              (1-2): 2 x None\n",
       "            )\n",
       "            (1): FeedForward(\n",
       "              (ff): Sequential(\n",
       "                (0): Sequential(\n",
       "                  (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                  (1): GELU(approximate='none')\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (2): Residual()\n",
       "          )\n",
       "        )\n",
       "        (adaptive_mlp): Identity()\n",
       "        (final_norm): LayerNorm(\n",
       "          (ln): LayerNorm((384,), eps=1e-05, elementwise_affine=False)\n",
       "        )\n",
       "        (skip_combines): ModuleList(\n",
       "          (0-11): 12 x None\n",
       "        )\n",
       "      )\n",
       "      (to_logits): Linear(in_features=384, out_features=51, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MetaLearningTask.load_from_checkpoint(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A:\n",
    "    d = 10\n",
    "    def __init__(self, a):\n",
    "        self.a = a\n",
    "    \n",
    "    def xd(self):\n",
    "        return self.__class__\n",
    "\n",
    "class B(A):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.B at 0x7fdfeb59a4d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.xd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
